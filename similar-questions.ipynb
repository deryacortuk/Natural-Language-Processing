{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c19401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Derya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Derya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Derya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import string\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28fa205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv('C:/Users/Derya/Downloads/train.csv')\n",
    "train1=train.copy()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79a6523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1=train1.iloc[:,[2,4]]\n",
    "Q2=train1.iloc[:,[1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec83eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame( np.concatenate( (Q2.values, Q1.values), axis=0 ) )\n",
    "df.columns = ['id','question' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df2c0a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808575</th>\n",
       "      <td>379845</td>\n",
       "      <td>How many keywords are there in PERL Programmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808576</th>\n",
       "      <td>155606</td>\n",
       "      <td>Is it true that there is life after death?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808577</th>\n",
       "      <td>537929</td>\n",
       "      <td>What's this coin?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808578</th>\n",
       "      <td>537931</td>\n",
       "      <td>I am having little hairfall problem but I want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808579</th>\n",
       "      <td>537933</td>\n",
       "      <td>What is it like to have sex with your cousin?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>808580 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                           question\n",
       "0            1  What is the step by step guide to invest in sh...\n",
       "1            3  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2            5  How can I increase the speed of my internet co...\n",
       "3            7  Why am I mentally very lonely? How can I solve...\n",
       "4            9  Which one dissolve in water quikly sugar, salt...\n",
       "...        ...                                                ...\n",
       "808575  379845  How many keywords are there in PERL Programmin...\n",
       "808576  155606         Is it true that there is life after death?\n",
       "808577  537929                                  What's this coin?\n",
       "808578  537931  I am having little hairfall problem but I want...\n",
       "808579  537933      What is it like to have sex with your cousin?\n",
       "\n",
       "[808580 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdd54e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be3e0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_quora=[word_tokenize(str(wrd)) for wrd in df.question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6542a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Quora_training_data=[TaggedDocument(d, [i]) for i, d in enumerate(tok_quora)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d1552",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model = Doc2Vec(Quora_training_data, vector_size = 100, window = 5, min_count = 3, epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e533600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_embeddings(model,tokens):\n",
    "    tokens = [x for x in word_tokenize(tokens) if x in list(doc_model.wv.vocab)]\n",
    " \n",
    "    if len(tokens)>=1:\n",
    "        return doc_model.infer_vector(tokens)\n",
    "    else:\n",
    "        return np.array([0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings=[]\n",
    "for w in df.question:\n",
    "    doc_embeddings.append(list(fetch_embeddings(doc_model, w)))\n",
    "#conveting it into array\n",
    "doc_embeddings=np.asarray(doc_embeddings)\n",
    "#shape\n",
    "Shape=(10000,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bcc8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a28f8ba1cea423891b3763f7da55e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7f70f/.gitattributes:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ef68ecdc3a42849eff0ef94d7f6252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da08e25b719d4520967e0f504b9e2e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)da6557f70f/README.md:   0%|          | 0.00/3.70k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624b7c0effdd4719815dbb22a72e8fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)6557f70f/config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494c7d6440bb485c94aa8ac19d608160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437bb7be9d8f4e14bd9a02471a87fffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ad2a470de245e1957416137c49d7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ac6cc8daa64fbd99063e41a449e206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5566a732c3b14aa2b296938a3c92d7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7f70f/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c283705f8b2242e4af4754220a16f8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33022eacf22e40919f6c59697fe81999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)da6557f70f/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130806f6f56144878423e49f83ca8d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)557f70f/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "x=[i for i in df.question]\n",
    "#lets get embeddings for each question\n",
    "sentence_embeddings_BERT= sbert_model.encode(x)\n",
    "#lets see the shape\n",
    "sentence_embeddings_BERT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1817251",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ccb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel\n",
    "tok_gpt= OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "\n",
    "model_gpt= OpenAIGPTModel.from_pretrained('openai-gpt')\n",
    "model_gpt.eval()\n",
    "\n",
    "def Fetch_gpt_vectors(question):\n",
    " #tokenize words\n",
    "    words = word_tokenize(question)\n",
    "    emb = np.zeros((1,768))\n",
    " #get vectore for each word\n",
    "    for word in words:\n",
    "        w= tok_gpt.tokenize(word)\n",
    "    indexed_words = tok_gpt.convert_tokens_to_ids(w)\n",
    "    tns_word = torch.tensor([indexed_words])\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    " #get mean vector\n",
    "           emb += np.array(torch.mean(model_gpt(tns_word),1))\n",
    "        except Exception as e:\n",
    "           continue\n",
    "    emb /= len(words)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ae8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_emb = np.zeros((1000, 768))\n",
    "for v in range(1000):\n",
    "    txt = df.loc[v,'question']\n",
    "    gpt_emb[v] = Fetch_gpt_vectors(txt)\n",
    "gpt_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a38aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608780d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1,vec2):\n",
    " \n",
    "    return dot(vec1, vec2)/(norm(vec1)*norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cce47ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_questions(user,embeddings,df):\n",
    "    x=cosine_similarity(user,embeddings).tolist()[0]\n",
    "    temp_list=list(x)\n",
    " #sorting\n",
    "    sort_res = sorted(range(len(x)), key = lambda sub: x[sub])[:]\n",
    "    sim_score=[temp_list[i] for i in reversed(sort_res)]\n",
    " #print\n",
    "    print(sort_res[0:5])\n",
    " #index fetching\n",
    "    L=[]\n",
    "    for i in reversed(sort_res):\n",
    "        L.append(i)\n",
    " #get the index from dataframe\n",
    "    return df.iloc[L[0:5], [0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_vector(query,model):\n",
    "    print(query)\n",
    " #Doc2vec model\n",
    "    if model=='Doc2Vec':\n",
    "        k=fetch_embeddings(doc_model,query)\n",
    "        k=k.reshape(1, -1)\n",
    "    elif model=='BERT':\n",
    "        k=sbert_model.encode(str(query))\n",
    "        k=k.reshape(1, -1)\n",
    " # gpt model\n",
    "    elif model=='GPT':\n",
    "        k=Fetch_gpt_vectors(query)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_questions(get_input_vector('How is Narendra Modi as a person?','Doc2Vec'),doc_embeddings,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027034c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_questions(get_input_vector('How is Narendra Modi as a person?','GPT'),gpt_emb,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829467fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_questions(get_input_vector('How is Narendra Modi as a person?','BERT'),sentence_embeddings_BERT,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a147c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3126ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/Derya/Downloads/train.csv')\n",
    "data = train.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32239082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_process(input_text):\n",
    " # Removing punctuation from input text\n",
    "    input_text = ''.join([x for x in input_text if x not in punctuation])\n",
    " # Cleaning the text\n",
    "    input_text = re.sub(r\"[^A-Za-z0-9]\", \" \", input_text)\n",
    "    input_text = re.sub(r\"\\'s\", \" \", input_text)\n",
    " # remove stop words\n",
    "    input_text = input_text.split()\n",
    "    input_text = [x for x in input_text if not x in stop_words]\n",
    "    input_text = \" \".join(input_text)\n",
    " # Return a list of words\n",
    "    return(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['question1_cleaned'] = data.apply(lambda x: txt_process(x['question1']), axis = 1)\n",
    "data['question2_cleaned'] = data.apply(lambda x: txt_process(x['question2']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text = np.hstack([data.question1_cleaned, data.question2_cleaned])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(question_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eb71a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenizer_1'] = tokenizer.texts_to_sequences(data.question1_cleaned)\n",
    "data['tokenizer_2'] = tokenizer.texts_to_sequences(data.question2_cleaned)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenizer'] = data['tokenizer_1'] + data['tokenizer_2']\n",
    "m_len = 500\n",
    "\n",
    "max_token = np.max(data.tokenizer.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85567f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[['is_duplicate']]\n",
    "X = data[['tokenizer']]\n",
    "#padding X with a maximum length\n",
    "X = sequence.pad_sequences(X.tokenizer, maxlen = m_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70f0433",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb596b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "quora_model = Sequential()\n",
    "#adding embeedding layer\n",
    "quora_model.add(Embedding(70000, 64))\n",
    "#adding drop out layer\n",
    "quora_model.add(Dropout(0.15))\n",
    "#LSTM layer\n",
    "quora_model.add(LSTM(16))\n",
    "#adding sigmoid layer\n",
    "quora_model.add(Dense(1, activation = 'sigmoid'))\n",
    "#defining loss and optimizer\n",
    "quora_model.compile(loss='binary_crossentropy', optimizer='SGD',metrics=['accuracy'])\n",
    "quora_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quora_model.fit(X_train, y_train, epochs = 2, batch_size=64,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf5543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09974962",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_prediction=quora_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f174315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_prediction[tr_prediction>0.5]=1\n",
    "tr_prediction[tr_prediction<0.5]=0\n",
    "tr_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_true=y_train.values\n",
    "#accuracy\n",
    "Accuracy=sklearn.metrics.accuracy_score(np.array(tr_true), np.array(tr_prediction))\n",
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f891cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(tr_true, tr_prediction, target_names=['Not similar','similar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975219a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction=quora_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3318bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction[test_prediction>0.5]=1\n",
    "test_prediction[test_prediction<0.5]=0\n",
    "test_prediction\n",
    "#true values for test\n",
    "test_true=y_test.values\n",
    "# accuracy on test data\n",
    "Accuracy=sklearn.metrics.accuracy_score(np.array(test_true), np.array(test_prediction))\n",
    "print('Accuracy is %f'%(Accuracy*100)+' %')\n",
    "\n",
    "print(classification_report(test_true, test_prediction, target_names=['Not similar','similar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2d68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
