{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41e0d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ed4f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8386fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfminer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "140d859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mammoth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a4e709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install locationtagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85332eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "from pdfminer3.layout import LAParams\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from pdfminer3.pdfinterp import PDFResourceManager\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer3.converter import TextConverter\n",
    "import io\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import mammoth\n",
    "import locationtagger\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_extractor(path):\n",
    "    r_manager = PDFResourceManager()\n",
    "    output = io.StringIO()\n",
    "    converter = TextConverter(r_manager, output, laparams=LAParams())\n",
    "    p_interpreter = PDFPageInterpreter(r_manager, converter)\n",
    "    with open(path, 'rb') as file:\n",
    "        for page in PDFPage.get_pages(file,caching=True,check_extractable=True):\n",
    "            p_interpreter.process_page(page)\n",
    "            text = output.getvalue()\n",
    "    converter.close()\n",
    "    output.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(file_path):\n",
    "    fileTXT = []\n",
    "     # This for loop is for reading all the files in file_path mentioned in the function\n",
    "    for filename in os.listdir(file_path):\n",
    "     # If the document is in pdf format then this code will be executed\n",
    "    if(filename.endswith(\".pdf\")):\n",
    "        try:\n",
    "            fileTXT.append(pdf_extractor(file_path+filename)) # Here the pdf_extractor function is used to extract pdf file\n",
    "        except Exception:\n",
    "            print('Error reading pdf file :' + filename)\n",
    "     # If the document is in docx format then this code will be executed\n",
    "    if(filename.endswith(\".docx\")):\n",
    "        try:\n",
    "            with open(file_path + filename, \"rb\") as docx_file:\n",
    "                result = mammoth.extract_raw_text(docx_file)\n",
    "                text = result.value\n",
    "                fileTXT.append(text)\n",
    "        except IOError:\n",
    "            print('Error reading .docx file :')\n",
    "     # If the given document is in doc format then this loop will be executed\n",
    "    if(filename.endswith(\".doc\")):\n",
    "        try:\n",
    "            text = textract.process(file_path+filename).decode('utf-8')\n",
    "            fileTXT.append(text)\n",
    "        except Exception:\n",
    "            print('Error reading .doc file :' + filename)\n",
    "     # If the given file in txt format then this file will be executed\n",
    "    if(filename.endswith(\".txt\")):\n",
    "        try:\n",
    "            myfile = open(file_path+filename, \"rt\")\n",
    "            contents = myfile.read()\n",
    "            fileTXT.append(contents)\n",
    "        except Exception:\n",
    "            print('Error reading .txt file :' + filename)\n",
    "    return fileTXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68245a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(Txt):\n",
    "    sw = stopwords.words('english')\n",
    "    space_pattern = '\\s+'\n",
    "    special_letters = \"[^a-zA-Z#]\"\n",
    "    p_txt = []\n",
    "    for resume in Txt:\n",
    "        text = re.sub(space_pattern, ' ', resume) # Removes extra spaces\n",
    "        text = re.sub(special_letters, ' ', text) # Removes special characters\n",
    "        text = re.sub(r'[^\\w\\s]','',text) # Removes punctuations\n",
    "        text = text.split() # Splits the words in a text\n",
    "        text = [word for word in text if word.isalpha()] # Keeps alphabetic word\n",
    "        text = [w for w in text if w not in sw] # Removes stopwords\n",
    "        text = [item.lower() for item in text] # Lowercases the words\n",
    "        p_txt.append(\" \".join(text)) # Joins all the words back\n",
    "    return p_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28654894",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdTxt = read_files(\"jd_path\")\n",
    "jds = preprocessing(jdTxt)\n",
    "TXT = p_resumeTxt+jds\n",
    "# Finding TF-IDF score of all the resumes and JDs.\n",
    "tv = TfidfVectorizer(max_df=0.85,min_df=10,ngram_range=(1,3))\n",
    "# Converting TF-IDF to a DataFrame\n",
    "tfidf_wm = tv.fit_transform(TXT)\n",
    "tfidf_tokens = tv.get_feature_names()\n",
    "df_tfidfvect1 = pd.DataFrame(data = tfidf_wm.toarray(),columns = tfidf_tokens)\n",
    "print(\"\\nTD-IDF Vectorizer\\n\")\n",
    "print(df_tfidfvect1[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4138bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimrec = TruncatedSVD(n_components=30, n_iter=7, random_state=42)\n",
    "transformed = dimrec.fit_transform(df_tfidfvect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eca72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vl = transformed.tolist()\n",
    "# Converting list to DataFrame\n",
    "fr = pd.DataFrame(vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc08f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(df_tfidfvect1[0:len(resumeTxt)],df_tfidfvect1[len(resumeTxt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = []\n",
    "for i in range(1,len(jds)+1):\n",
    "    abc.append(f\"JD {i}\")\n",
    "# DataFrame of similarity score\n",
    "Data=pd.DataFrame(similarity,columns=abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2438d75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame({'Original Resume':resumeTxt})\n",
    "dt = pd.concat([Data,t],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number(text):\n",
    "    \n",
    " \n",
    "    pattern = re.compile(r'([+(]?\\d+[)\\-]?[ \\t\\r\\f\\v]*[(]?\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*)')\n",
    "     # findall finds the pattern defined in compile\n",
    "    pt = pattern.findall(text)\n",
    "     # sub replaces a pattern matching in the text\n",
    "    pt = [re.sub(r'[,.]', '', ah) for ah in pt if len(re.sub(r'[()\\-.,\\s+]', '', ah))>9]\n",
    "    pt = [re.sub(r'\\D$', '', ah).strip() for ah in pt]\n",
    "    pt = [ah for ah in pt if len(re.sub(r'\\D','',ah)) <= 15]\n",
    "    for ah in list(pt):        \n",
    "     # split splits a text\n",
    "        if len(ah.split('-')) > 3: continue\n",
    "            for x in ah.split(\"-\"):\n",
    "                try:\n",
    "     # isdigit checks whether the text is number or not\n",
    "                   if x.strip()[-4:].isdigit():\n",
    "            \n",
    "                      if int(x.strip()[-4:]) in range(1900, 2100):\n",
    "                          pt.remove(ah)\n",
    "                except: pass\n",
    "                    \n",
    "    number = None\n",
    "    number = list(set(pt))\n",
    "    return number              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_ID(text):\n",
    " # compile helps us to define a pattern for matching it in the text\n",
    "    r = re.compile(r'[A-Za-z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+')\n",
    "    return r.findall(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60940e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def location(text):\n",
    "    place_entity = locationtagger.find_locations(text=text)\n",
    "    return place_entity.cities\n",
    "dt['Location']=dt['Resume'].apply(lambda x: location(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompanyName(text):\n",
    " # for tagging each entity with its it’s labels\n",
    "    tokens = nlp(str(text))\n",
    "    x=[]\n",
    "    for ent in tokens.ents:\n",
    "        if ent.label_ == 'ORG':\n",
    "    return ent.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 500,background_color ='white',min_font_size = 10).generate(resumeTxt[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dc55aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 5), facecolor = None)\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d43151",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
